from docling_core.transforms.chunker import HierarchicalChunker
from docling.document_converter import DocumentConverter
from docling.chunking import HybridChunker
from pymilvus import MilvusClient
from pymilvus import model
import time # Import time for a small delay if needed after collection creation
import requests # Import the requests library for remote API calls

# --- Configuration ---
# Define the name of your Milvus collection
COLLECTION_NAME = "my_docling_rag_collection"
# The embedding dimension for 'paraphrase-albert-small-v2' is 768.
# This must match the output dimension of your embedding model.
EMBEDDING_DIM = 768
# Source document to convert and chunk
SOURCE_URL = "./AM69PSK.pdf"
# Number of results to retrieve from the search
SEARCH_LIMIT = 5 # Increased default search limit for potentially better context

# Remote LLM model configuration
REMOTE_LLM_URL = "https://semikong-semikong.apps.cluster-9dctl.9dctl.sandbox2233.opentlc.com/v1/chat/completions"
# !!! IMPORTANT: Replace "YOUR_AUTHORIZATION_TOKEN_HERE" with your actual token !!!
# Consolidating the token into a single raw string literal for robustness.
# Added .strip() to remove any potential leading/trailing whitespace.
AUTH_TOKEN = r"token".strip()
# LLM model identifier (might be used by the remote service to select the model)
LLM_MODEL = "semikong" # Assuming 'semikong' is the internal model name on the remote API

# --- Initialize Docling Components ---
print("Initializing Docling DocumentConverter and HierarchicalChunker...")
converter = DocumentConverter()
#chunker = HierarchicalChunker()
chunker = HybridChunker()
print("Docling components initialized.")

# --- Convert Document and Perform Chunking ---
print(f"Converting document from: {SOURCE_URL}")
try:
    doc = converter.convert(SOURCE_URL).document
    print("Document converted successfully.")
except Exception as e:
    print(f"Error converting document: {e}")
    print("Please ensure the source URL is accessible and Docling can process it.")
    exit()

print("Performing hierarchical chunking...")
texts = [chunk.text for chunk in chunker.chunk(doc)]
if not texts:
    print("No text chunks were generated. Please check the document and chunking logic.")
    exit()
print(f"Generated {len(texts)} text chunks.")

# --- Initialize Milvus Client ---
# Connect to Milvus. For local demo, using a file-based URI for embedded Milvus.
print("Initializing MilvusClient...")
milvus_client = MilvusClient(uri="./milvus_demo.db")
print("MilvusClient initialized.")

# --- Manage Milvus Collection ---
print(f"Checking if collection '{COLLECTION_NAME}' exists...")
if milvus_client.has_collection(COLLECTION_NAME):
    print(f"Collection '{COLLECTION_NAME}' already exists. Dropping it...")
    milvus_client.drop_collection(COLLECTION_NAME)
    print("Collection dropped.")

print(f"Creating new collection '{COLLECTION_NAME}' with dimension {EMBEDDING_DIM}...")
milvus_client.create_collection(
    collection_name=COLLECTION_NAME,
    dimension=EMBEDDING_DIM, # Corrected: Use the actual embedding dimension
    metric_type="IP",  # Inner product distance, suitable for normalized embeddings
    consistency_level="Strong", # Strong consistency ensures freshest data but might have higher latency
)
print("Collection created successfully.")

# Give Milvus a moment to set up the collection (optional, but can help prevent race conditions on very fast setups)
time.sleep(1)

# --- Generate Embeddings and Insert Data ---
print("Initializing embedding function (this may download a model if not cached)...")
# This will download a small embedding model "paraphrase-albert-small-v2" (~50MB).
# The embeddings generated by this model are 768-dimensional.
embedding_fn = model.DefaultEmbeddingFunction()
print("Embedding function initialized.")

print("Encoding text chunks into vectors...")
# Encode the text chunks into vectors.
# We use encode_documents as we are encoding multiple text documents for insertion.
data_vectors = embedding_fn.encode_documents(texts)

# Prepare data for insertion into Milvus.
# Each item in 'data_to_insert' will be a dictionary representing a row in the collection.
# It MUST contain 'vector' and 'text' fields (as defined in create_collection implicitly,
# Milvus's dynamic schema allows this). We'll add an 'id' for better retrieval/identification.
data_to_insert = [
    {"id": i, "vector": data_vectors[i], "text": texts[i]}
    for i in range(len(texts))
]

print(f"Inserting {len(data_to_insert)} entities into collection '{COLLECTION_NAME}'...")
try:
    insert_result = milvus_client.insert(
        collection_name=COLLECTION_NAME,
        data=data_to_insert
    )
    print("Data inserted successfully.")
    # print(f"Insert result: {insert_result}") # Uncomment to see detailed insert results
except Exception as e:
    print(f"Error inserting data into Milvus: {e}")
    exit()

# --- LLM Agent Generation Function ---
def generate_llm_response(query: str, context: str, llm_url: str, auth_token: str, model_name: str) -> str:
    """
    Generates a response using a remote LLM API based on the query and provided context.
    """
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {auth_token}" # Ensure 'Bearer ' has a space
    }

    # Print the Authorization header for debugging
    # print(f"DEBUG: Authorization Header: {headers['Authorization']}") # Uncomment for debugging

    messages = [
        {
            "role": "user",
            "content": f"Using the following context, answer the question. If the answer is not in the context, state that you cannot answer from the provided information.\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:"
        }
    ]

    payload = {
        "model": model_name,
        "messages": messages,
        "max_tokens": 500, # Added for better control of response length
        "temperature": 0.7 # Added for controlling creativity
    }

    print(f"\nSending request to remote LLM at {llm_url}...")
    try:
        # Send a POST request to the remote LLM endpoint
        # verify=False is kept as it might be necessary for sandbox environments with self-signed certificates
        response = requests.post(llm_url, headers=headers, json=payload, verify=False)
        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)

        # Expected response structure for OpenAI Chat Completions API
        response_json = response.json()
        if 'choices' in response_json and len(response_json['choices']) > 0 and \
           'message' in response_json['choices'][0] and \
           'content' in response_json['choices'][0]['message']:
            return response_json['choices'][0]['message']['content']
        else:
            print(f"Unexpected response format from remote LLM: {response_json}")
            return "Failed to parse response from remote LLM."

    except requests.exceptions.RequestException as e:
        print(f"Error communicating with remote LLM: {e}")
        # Attempt to print response content if available for debugging
        if hasattr(e, 'response') and e.response is not None:
            print(f"Server response content: {e.response.text}")
        return "Failed to generate response from remote LLM."
    except Exception as e:
        print(f"An unexpected error occurred during remote LLM call: {e}")
        return "An unexpected error occurred."

# --- Main CLI Loop for Agent Interaction ---
print("\n--- RAG Agent Ready ---")
print("Type your query and press Enter. Type 'exit' or 'quit' to end the session.")

while True:
    user_query = input("\nYour Query: ").strip()

    if user_query.lower() in ["exit", "quit"]:
        print("Exiting RAG Agent. Goodbye!")
        break

    # --- Perform Vector Search for the current query ---
    print(f"Encoding search query: '{user_query}'...")
    query_vectors = embedding_fn.encode_queries([user_query])

    print("Performing vector search...")
    retrieved_context = ""
    try:
        res = milvus_client.search(
            collection_name=COLLECTION_NAME,
            data=query_vectors,
            limit=SEARCH_LIMIT,
            output_fields=["id", "text"],
        )
        print("\n--- Search Results (Retrieved Context) ---")
        retrieved_texts = []
        for hit in res[0]:
            print(f"  ID: {hit['id']}")
            print(f"  Distance (IP): {hit['distance']:.4f}")
            print(f"  Text: {hit['entity']['text'][:150]}...") # Limit text snippet for display
            retrieved_texts.append(hit['entity']['text'])
            print("-" * 20)

        retrieved_context = "\n\n".join(retrieved_texts)
        if not retrieved_context:
            print("No context retrieved for the query.")
        print("---------------------------------------")

    except Exception as e:
        print(f"Error during search: {e}")
        print("Proceeding with empty context for LLM generation.")
        retrieved_context = "" # Ensure context is empty on search error

    no_context_answer = generate_llm_response(user_query, "", REMOTE_LLM_URL, AUTH_TOKEN, LLM_MODEL)
    print("\n--- Semikong-70B answer ---")
    print(no_context_answer)
    print("--------------------------")
    
    
    # Generate the final answer using the remote LLM and retrieved context
    with_context_answer = generate_llm_response(user_query, retrieved_context, REMOTE_LLM_URL, AUTH_TOKEN, LLM_MODEL)

    print("\n--- Semikong-70B + Docling + RAG(Milvus) with_context_answer ---")
    print(with_context_answer)
    print("--------------------------")

# --- Clean Up (Optional) ---
# Uncomment the following lines to drop the collection after execution if you want to clean up.
# print(f"Dropping collection '{COLLECTION_NAME}' for cleanup...")
# milvus_client.drop_collection(COLLECTION_NAME)
# print("Collection dropped.")
